{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ActionEngine Documentation","text":"<p>ActionEngine is a tool-call orchestration framework for agentic workflows. Specifically, it is designed for agents that interact with a set of complex, interdependent API endpoints.(1)</p> <ol> <li>Wrapper libraries for popular websites such as PyGithub, python-youtube etc. are examples of such sets of API endpoints.</li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy to use - Minimalistic interfaces inspired by FastAPI and Typer</li> <li>Type Safety - Designed to be fully compatible with type checkers such as mypy</li> <li>Zero LLM wrapper - Pick your favorite LLM package such as <code>openai</code> <code>liteLLM</code> <code>aisuite</code> and use it directly with ActionEngine</li> <li>State Management: Engine manages the global state, ensuring only the right variables are passed to the actions when calling</li> <li>Tool Filtering: Tools that are not compatible with the current state are automatically filtered out</li> </ul>"},{"location":"#installation","title":"Installation","text":"uvpip <pre><code>uv add action-engine\n</code></pre> <pre><code>pip install action-engine\n</code></pre>"},{"location":"#how-does-action-engine-work","title":"How Does Action Engine Work?","text":""},{"location":"#motivation","title":"Motivation","text":"<p>Agent frameworks we have today are designed to use APIs where the arguments can be inferred by a LLM using its in-weight knowledge: <pre><code>from duckduckgo_search import DDGS\n\nresults = DDGS().text( # Search using the DuckDuckGo API\n    \"python programming\", # This can be inferred by an LLM! \n    max_results=5\n)\n</code></pre> However, they are less suitable with APIs where an LLM's in-weight knowledge is not enough: <pre><code>from pyyoutube import Client\nclient = Client(...)\nclient.channels.list( # Get youtube channel info\n    channel_id=\"UC_x5XG1OV2P6uZZ5FSM9Ttw\" # This cannot be inferred by an LLM!\n)\n</code></pre></p> <p>Usually, the second type of APIs are a part of a larger system of APIs where the non-inferrable apis are dependent on some directly inferrable apis: <pre><code>from bilibili_api import search, comment\nfrom bilibili_api.video import Video\n\nvid = await search.search_by_type(\n    \"cute cate videos\" # This can be inferred by an LLM!\n)[\"result\"][i]\n\ncomment.send_comment( \n    text=f\"I like this video!\",\n    oid=Video(bvid=vid[\"bvid\"]).get_aid() # This value depends on the previous API call!\n)\n</code></pre></p> <p>ActionEngine is designed to handle such complex, interdependent APIs for agentic workflows. It does this by 1. automatically filter out actions that are not compatible with the current state 2. automatically fills the input arguments when the next action is chosen.    </p> <p>Only two things are required to use Action Engine: 1. A selector that selects the agent's next action 2. A set of actions that the agent can perform</p>"},{"location":"#example-github-issue-summary-bot","title":"Example: Github Issue Summary Bot","text":"<pre><code># imports omitted for brevity\n\nclass Base(BaseModel):\n    g: Github\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    history: list[str]\n    llm: Client\n    summary: str = \"\"\n\ndef action_selector(base: Base, actions: list[Action]) -&gt; Action:\n    target = \"Goal: browse github repositories, for each repo, get three issues. When you are done, generate a summary. \\n\"\n    postfix = \"Pick your next action based on the past actions, give the index and nothing else.\\n\"\n    query = target + indexed_str(\"Past actions\", base.history) + indexed_str(\"Possible actions\", actions) + postfix\n    response = base.llm.chat(message=query).text\n    return actions[int(response)]\n\nengine = Engine(\n    base_state_type=Base,\n    base_action_selector=action_selector,\n)\n\n@engine.action()\ndef browse_repo(base: Base) -&gt; Annotated[Repository, Tag(\"repo\")]:\n    message = indexed_str(\"Your Past Actions\", base.history) + \"Give me a keyword about ai and nothing else:\"\n    query = base.llm.chat(message=message).text\n    repo = base.g.search_repositories(query=query).get_page(0)[0]\n    base.history.append(f\"Browsing repository {repo.name}: {repo.description}\")\n    return repo\n\n@engine.action()\ndef get_issue(base: Base, repo: Repository) -&gt; None:\n    issues = repo.get_issues().get_page(0)[:10]\n    history = indexed_str(\"Your Past Actions\", base.history)\n    postfix = \"Your response should be a number and nothing else.\"\n    response = base.llm.chat(message=history + \"Now, pick a new issue:\" + indexed_str(\"Issues\", issues) + postfix).text\n    base.history.append(f\"Retrieved issue: {issues[int(response)].title}\")\n\n@engine.action(terminal=True)\ndef summarize(base: Base) -&gt; None:\n    base.summary = base.llm.chat(message=\"Generate a summary of the repos and issues:\" + \"\\n\".join(base.history)).text\n\nif __name__ == \"__main__\":\n    base = Base(\n        history=[],\n        llm=Client(os.environ.get(\"COHERE_API_KEY\")),\n        g=Github(auth=Auth.Token(os.environ[\"GITHUB_TOKEN\"]))\n    )\n    engine.run(base)\n    print(base.summary)\n</code></pre>"}]}